{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Fc-cAAWOp4k"
   },
   "source": [
    "# Support Vector Machines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Introduction</h2>\n",
    "There are two parts to this lab session. \n",
    "\n",
    "1. A part on PEGASOS where you are asked to implement the stochastic version of PEGASOS\n",
    "\n",
    "2. A part on features mapping\n",
    "\n",
    "Data are 2D. You can visualize them by running the following cell (red dots are labeled -1 and green dots are labeled 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from math import sqrt, cos, pi\n",
    "import pandas as pd\n",
    "\n",
    "# print the datasets and their delimiters\n",
    "datasets = [\"data/dataset\" + str(x) + \".txt\" for x in range(1, 5)]\n",
    "for dataset in datasets :\n",
    "    print(dataset)\n",
    "    points_df=pd.read_csv(dataset,delimiter=' ')\n",
    "    plt.plot(points_df.x_1[points_df.y==-1],points_df.x_2[points_df.y==-1],'r.') # points whose target y is -1\n",
    "    plt.plot(points_df.x_1[points_df.y==1],points_df.x_2[points_df.y==1],'g.') # points whose target y is +1\n",
    "    # draw exact delimiter\n",
    "    if dataset == datasets[0]:\n",
    "        plt.plot(np.array([0,40])/10-2,-np.array([0,40])/10+2, 'b--')\n",
    "    elif dataset == datasets[1]:\n",
    "        plt.plot([x/10-1.5 for x in range(30)], [0.5-((x/10)-1.5)**3 for x in range(30)], 'b--')\n",
    "    elif dataset == datasets[2]:\n",
    "        plt.plot([x/(4*17.321)-0.866 for x in range(120)]+[np.sqrt(3)/2], [np.sqrt(1.5-2*(x/(4*17.321)-0.866)**2) for x in range(120)]+[0], 'b--')\n",
    "        plt.plot([x/(4*17.321)-0.866 for x in range(120)]+[np.sqrt(3)/2], [-np.sqrt(1.5-2*(x/(4*17.321)-0.866)**2) for x in range(120)]+[0], 'b--')\n",
    "    elif dataset == datasets[3]:\n",
    "        plt.plot([x/8 for x in range(17)] + [2 for x in range(17)] + [(2-x/8) for x in range(17)] + [0 for x in range(17)],\n",
    "             [0 for y in range(17)] + [y/8 for y in range(17)] + [2 for y in range(17)] + [(2-y/8) for y in range(17)], 'b--')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 0.1\n",
    "According to the above plots, do these datasets seem to be linearly separable? On which of these datasets do you expect a linear SVM to perform well? On which ones do you think it will perform badly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part 1: PEGASOS</h2>\n",
    "\n",
    "<h3>How PEGASOS works</h3>\n",
    "\n",
    "<i>That sub-part gives some intuition about how PEGASOS can actually find the solution of a problem cast by a linear SVM. Reading it is not required to do the lab, but we encourage you to have a look at it as it may help you for your implementation.</i>\n",
    "\n",
    "Let us assume that given a set $S = \\lbrace (x_i, y_i) \\rbrace$ where $x_i \\in \\mathbb{R}^n$ and $y_i \\in \\lbrace -1, 1 \\rbrace$, we want to find a solution of the following problem\n",
    "\n",
    "$$\\min_{w, b} \\frac{1}{2} \\Vert w \\Vert^2 + C \\sum_i \\max(0, 1-y_i(w^Tx_i + b))$$\n",
    "\n",
    "where $C$ is a non-zero hyperparameter controlling the \"softness\" of the margin (the lower the $C$ the softer the margin), or equivalently\n",
    "\n",
    "$$\\min_{w, b} \\frac{\\lambda}{2} \\Vert w \\Vert^2 + \\sum_i \\max(0, 1-y_i(w^Tx_i + b))$$\n",
    "\n",
    "where $\\lambda$ replaces $C$ (the higher the $\\lambda$ the softer the margin). We can get rid of the bias term $b$ by adding a constant feature to all the $x_i$ (we can assume without loss of generality that the value of the constant is 1) and by replacing $w$ by the concatenation of $w$ and $b$. In that setup, the optimization problem becomes (with $\\tilde{w} = [w; b]$ and $\\tilde{x_i} = [x_i; 1]$)\n",
    "\n",
    "$$\\min_{\\tilde{w}} \\frac{\\lambda}{2} \\Vert \\tilde{w} \\Vert^2 + \\sum_i \\max(0, 1-y_i(\\tilde{w}^T\\tilde{x_i})).$$\n",
    "\n",
    "You can notice that the optimization problem we obtained is slightly different from the one we stated before, as $\\Vert \\tilde{w} \\Vert^2$ should be replaced by $\\Vert \\tilde{w} \\Vert^2 - b^2$. However, in practice, that difference has very few importance. Therefore in the following we will assume that $x_i \\in \\mathbb{R}^{n+1}$, $w \\in \\mathbb{R}^{n+1}$, and that the last feature of $x_i$ is always equal to 1. As we will work on the stochastic variant of PEGASOS, we will consider the following objective:\n",
    "\n",
    "$$\\min_{w_t} \\frac{\\lambda}{2} \\Vert w_t \\Vert^2 + \\max(0, 1-y_{i_t}(w_t^Tx_{i_t}))$$\n",
    "\n",
    "where $(x_{i_t}, y_{i_t})$ is a sample drawn randomly at iteration $t$ from $S$. The sub-gradient of the above objective is then given by:\n",
    "\n",
    "$$\\nabla w_t = \\lambda w_t - \\phi_I(x_{i_t}, y_{i_t}) y_{i_t}x_{i_t}$$\n",
    "\n",
    "where $\\phi_I$ is the indicator function of the set $I = \\lbrace (x, y) \\in R^d \\times \\lbrace -1, 1 \\rbrace : y w^Tx < 1 \\rbrace$ ($\\phi_I(x, y) = 1$ if $(x, y) \\in I$ and $\\phi_I(x, y) = 0$ otherwise). Then, we compute $w_{t+1}$ using the following update formula:\n",
    "\n",
    "$$w_{t+1} = w_t - \\eta_t \\nabla w_t$$\n",
    "\n",
    "where $\\eta_t = 1/(\\lambda t)$. The algorithm stops after a predefined number of iterations. As you can understand, PEGASOS uses a stochastic gradient descent with learning rate $\\eta_t$ to find the optimal parameters of an SVM. The next sub-part gives you a pseudo-code of PEGASOS. Your role will be to implement it based on that pseudo-code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Pseudo-code of PEGASOS</h3>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "INPUTS: S = {(x_1, y_1), ..., (x_n, y_n)}   # Dataset (x_i are data vectors and y_i are data labels in {-1, 1})\n",
    "        lambda   # Predefined hyperparameter\n",
    "        T   # Total number of iterations\n",
    "\n",
    "PEGASOS(S, lambda, T):\n",
    "    w_1 = [0, ..., 0]\n",
    "    For t in {1, ..., T}:\n",
    "        Pick (x, y) in S uniformly at random\n",
    "        Set eta_t = 1/(lambda*t)\n",
    "        If y*dot_product(w_t, x) < 1:\n",
    "            Set w_{t+1} = w_t - eta_t*(lambda*w_t - y*x)\n",
    "        Else:\n",
    "            Set w_{t+1} = w_t - eta_t*lambda*w_t\n",
    "    Return w_T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task will now be to implement PEGASOS. An SVM class has been defined in utils.py. You are strongly encouraged to read it carefully to understand it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1\n",
    "Complete the following implementation of PEGASOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import SVM, Dataset\n",
    "\n",
    "def train(self, n_iterations, lambda_w, print_every_k=None,verbose=False):\n",
    "    '''\n",
    "    Train function for an SVM model using PEGASOS.\n",
    "    If verbose is TRUE print the accuracy every 'print_every_k' iterations\n",
    "\n",
    "    INPUTS:\n",
    "    - self containing in particular:\n",
    "        self.train_dataset with inside\n",
    "            self.train_dataset.input -> inputs in a numpy NxD array, N number of sample, D dimensions\n",
    "            self.train_dataset.output -> outputs in a numpy X array\n",
    "        self.w -> weights of the model initialized at random before. Length D+1, because 1 is the bias\n",
    "    - n_iterations -> total number of epochs\n",
    "    - lambda_w -> lambda in PEGASOS\n",
    "    - print_every_k -> compute the accuracy of the model every 'print_every_k' epochs\n",
    "    - verbose -> if TRUE, in addition of computing the accuracy, the model also print it every 'print_every_k' epochs\n",
    "    '''\n",
    "\n",
    "    \n",
    "    if not print_every_k:\n",
    "        print_every_k = max(1, int(n_iterations/50))\n",
    "        self.print_step=print_every_k\n",
    "        # For n_iterations epochs\n",
    "        for i in range(n_iterations): # equivalent for t in {1,...,T}\n",
    "        \n",
    "        # ! REMEMBER TO UPDATE self.w AT THE END OF EVERY ITERATION\n",
    "        # ! self.train_dataset.input DOES NOT CONTAIN THE BIAS : a '1' in the dimension D+1\n",
    "            # REMBER TO ADD THE '1' OF THE BIAS TO YOUR x VECTOR OR YOU WILL HAVE A SIZE MISMATCH\n",
    "            # ADD THE BIAS IN THE LAST POSITION OF YOUR x VECTOR, OTHERWISE THE REST OF THE TRAIN WILL BE WRONG\n",
    "            # x VECTOR OF [self.train_dataset.input[sample,:], 1]\n",
    "        \n",
    "        ################ YOUR CODE HERE #################\n",
    "        \n",
    "        \n",
    "        ################ END OF YOUR CODE ###############\n",
    "        \n",
    "            if not i%print_every_k:\n",
    "                if verbose:\n",
    "                    print(\"Epoch: \", i+1, \" out of \", n_iterations)\n",
    "                    self.print_accuracy()\n",
    "                else:\n",
    "                    self.compute_accuracy()\n",
    "                    \n",
    "SVM.train = train                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Exercise 1.2\n",
    "Test your implementation on the four datasets (train your SVM during 1500 iterations with $\\lambda = 0.0001$).\n",
    "You have to:\n",
    "\n",
    "1. create 2 objects Dataset, with the train dataset and test dataset for each dataset in the folder data\n",
    "2. initialize your SVM model with the 2 objects\n",
    "3. use the train function of your SVM object to train it\n",
    "4. use the make_plot function of your SVM object to make the plot of its accuracy\n",
    "5. use the function print_accuracy of your SVM object to see the final training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.1\n",
    "Do you find the results that you forecast in Question 0.1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part 2: Features Maps</h2>\n",
    "\n",
    "As you know, it is possible to use the \"Kernel Trick\" to separate non-linearly separable datasets with SVMs. This trick is meant to be applied on the dual problem. As PEGASOS is based on the primal problem, the Kernel Trick cannot be used easily with PEGASOS. However, there is a method to approximate the feature space corresponding to a given kernel. In this part, you will implement and test that method. You will not be asked to dig into the mathematical theory behind that method, but if you wish, you can find more informations in the paper <i>Random Features for Large-Scale Kernel Machines</i> by Ali Rahimi and Ben Recht [1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximating the Radial Basis Function (RBF) kernel\n",
    "\n",
    "The Radial Basis Function $K_\\gamma$ is one of the most common kernels used with SVMs. It is defined as follows ($x, y \\in \\mathbb{R}^d$):\n",
    "\n",
    "$$ K_\\gamma(x, y) = \\exp(-\\gamma \\Vert x - y \\Vert^2).$$\n",
    "\n",
    "Its feature space is of infinite dimension. However, you can approximate it according to the following algorithm (details in [1]):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "INPUTS:\n",
    "    S = {(x1, y1), ..., (xn, yn)}   # Dataset\n",
    "    mean = zeros(d)\n",
    "    cov = 2*d*gamma*eye(d)\n",
    "    D = wanted dimension of the approximation of the RBF feature space\n",
    "\n",
    "FEATURES_MAP(S, mean, sigma, D):\n",
    "    S' = {}\n",
    "    p ~ multivariate_normal_distribution(mean, cov)\n",
    "    Draw D iid samples w1, ..., wD from p and D iid samples b1, ..., bD uniformly in [0, 2*pi]\n",
    "    For all x in S:\n",
    "        z = [cos(dot_product(w1, x) + b1), ..., cos(dot_product(wD, x) + bD)]\n",
    "        z *= sqrt(2/D)\n",
    "        Add z to S'\n",
    "    Return S'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pseudo-code above transforms data from its original space to a new feature space approximating the feature space corresponding to the RBF kernel. You will now implement that pseudo-code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.1\n",
    "Complete the following implementation of the FeaturesMap class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import Dataset\n",
    "from math import cos, pi\n",
    "import random\n",
    "\n",
    "class FeaturesMap:\n",
    "\n",
    "    def __init__(self, D, d, gamma):\n",
    "        '''\n",
    "        INPUT:\n",
    "        - d : old dimensions\n",
    "        - D : new dimensions\n",
    "        - gamma : softness/hardness parameter\n",
    "        '''\n",
    "        mean = np.zeros(d)\n",
    "        cov = 2*d*gamma*np.eye(d)\n",
    "        self.D = D\n",
    "        self.w = np.array([np.random.multivariate_normal(mean, cov) for _ in range(D)])\n",
    "        self.b = np.array([random.random()*2*pi for _ in range(D)])\n",
    "\n",
    "    def __call__(self, dataset):\n",
    "        outputs = dataset.output\n",
    "        S_prime = [] # S'\n",
    "        S=dataset.input\n",
    "        for x in S:\n",
    "            # compute z and append the values to S_prime keeping it as a list if you want\n",
    "            \n",
    "            ########## YOUR CODE HERE #################\n",
    "            \n",
    "            \n",
    "            ########## END OF YOUR CODE HERE ##########\n",
    "            \n",
    "        new_data = Dataset(input_size=self.D, length=dataset.len)\n",
    "        new_data.input = np.array(S_prime)\n",
    "        new_data.output = outputs\n",
    "        \n",
    "        return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.2\n",
    "Test your implementation on the four artificial datasets. Train your SVM during 1500 iterations with $\\lambda = 0.0001$. The dimension of the feature space is 200 and $\\gamma = 1$.\n",
    "\n",
    "1. Create Dataset() objects for each train and test dataset\n",
    "2. Move each train and test Dataset() object using FeaturesMap()\n",
    "3. Init and train SVM with featured map model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations=1500\n",
    "lambda_w = 0.0001\n",
    "D=200\n",
    "gamma=1\n",
    "\n",
    "for i in range(1,5):\n",
    "    print(\"\\n\\nDataset\", i)\n",
    "    datafile = \"data/dataset\"+str(i)+\"_train.txt\"\n",
    "    train_data = Dataset(datafile)\n",
    "    test_datafile = \"data/dataset\"+str(i)+\"_test.txt\"\n",
    "    test_data = Dataset(test_datafile)\n",
    "    \n",
    "    d=train_data.input_size\n",
    "\n",
    "    features_map = FeaturesMap(D, d, gamma)\n",
    "    new_train_data = features_map(train_data)\n",
    "    new_test_data = features_map(test_data)\n",
    "    \n",
    "    # fit and train your SVM model using new_train_data and  new_test_data\n",
    "    # then use make_plot() of your SVM object\n",
    "    # then use print_accuracy() of your SVM object\n",
    "    \n",
    "    ############# YOUR CODE #############\n",
    "    \n",
    "    \n",
    "    ########## END YOUR CODE ############ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.1 \n",
    "How do your results change with respect to what you found in Question 1.1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The role of $\\gamma$ in the RBF kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.3\n",
    "Do the same tests with $\\gamma = 10^{-6}$. What do you notice? How can you explain your results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.4\n",
    "Do the same tests with $\\gamma = 10^{4}$. What do you notice? How can you explain your results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
