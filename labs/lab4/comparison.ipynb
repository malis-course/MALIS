{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0871aa8-4448-4d01-9586-c4de645ad9a9",
   "metadata": {},
   "source": [
    "# Comparing classifiers\n",
    "\n",
    "In this part you will compare the classification algorithms seen in this and the previous labs of MALIS course:\n",
    "\n",
    "1. K nearest neighbors\n",
    "2. Logistic regression\n",
    "3. Neural Networks\n",
    "4. Support Vector Machine (linear)\n",
    "5. SVM with RBF kernel\n",
    "6. Classification Tree\n",
    "\n",
    "You have only to run the cells and answer the questions you can find after the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fea71e-2670-433f-a1c7-38aac8a830fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from sklearn.exceptions import ConvergenceWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1cb889-c44b-4ba4-a0cc-a25530a96034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code is not done from scratch, but it is realized using :\n",
    "# https://github.com/amueller/introduction_to_ml_with_python/blob/master/02-supervised-learning.ipynb\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "# classifier names and classes :\n",
    "names = [\"K Nearest Neighbors\", \"Logistic Regression\", \"Linear SVM\", \"RBF SVM\", \n",
    "         \"Decision Tree\", \"NN 1 hidden layer\",\"NN 2 hidden layers\"]\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    LogisticRegression(solver='liblinear', max_iter=400),\n",
    "    SVC(kernel=\"linear\", C=2000),\n",
    "    SVC(gamma=2, C=1),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    MLPClassifier(learning_rate_init=0.05, max_iter=3000, hidden_layer_sizes=(10), batch_size=60, learning_rate='constant',  activation='logistic', solver='sgd', tol=-10.),\n",
    "    MLPClassifier(learning_rate_init=0.05, max_iter=4000, hidden_layer_sizes=(10, 10), batch_size=60, learning_rate='constant', activation='logistic', solver='sgd', tol=-10.)]\n",
    "\n",
    "# set used to train and test\n",
    "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                           random_state=1, n_clusters_per_class=1)\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable = (X, y)\n",
    "\n",
    "datasets = [make_moons(noise=0.3, random_state=0),\n",
    "            make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "            linearly_separable\n",
    "            ]\n",
    "\n",
    "figure = plt.figure(figsize=(27, 9))\n",
    "\n",
    "i = 1\n",
    "# iterate over datasets\n",
    "for ds_cnt, ds in enumerate(datasets):\n",
    "    # preprocess dataset, split into training and test part\n",
    "    X, y = ds\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    # split in train and test\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=.4, random_state=42)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    \n",
    "    # plot the training dataset first\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    ax = plt.subplot(len(datasets), len(classifiers) + 2, i)\n",
    "    if ds_cnt == 0:\n",
    "        ax.set_title(\"Training input data\",size=20)\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "               edgecolors='k')\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "    # plot the test dataset\n",
    "    ax = plt.subplot(len(datasets), len(classifiers) + 2, i)\n",
    "    if ds_cnt == 0:\n",
    "        ax.set_title(\"Test data\",size=20)\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
    "               edgecolors='k')\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 2, i)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        score = clf.score(X_test, y_test)\n",
    "\n",
    "        # Plot the decision boundary. For that, we will assign a color to each\n",
    "        # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "        if hasattr(clf, \"decision_function\"):\n",
    "            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "            Z = np.where(Z>=np.mean(Z),1,0)\n",
    "        else:\n",
    "            Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])      \n",
    "\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        ax.pcolormesh(xx, yy, Z, cmap=cm, alpha=0.9)\n",
    "\n",
    "        # Plot the training points\n",
    "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors='k')\n",
    "\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        \n",
    "        # Plot the name of the classifier in each column\n",
    "        if ds_cnt == 0:\n",
    "            ax.set_title(name,size=20)\n",
    "        \n",
    "        # Plot the test accuracy\n",
    "        ax.set_xlabel((' Test accuracy : %.1f %%' % (100*score)),size=15)\n",
    "        i += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c85518-1d7d-4548-98b9-1ec421b69556",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Comment the results obtained. Use the following questions to help you.\n",
    "1. Which classifiers performs poorly? In which datasets? Why?\n",
    "2. Do you think some classifiers are overfitting?\n",
    "    1. If yes: which ones? in which datasets? which elements allow you to say that?\n",
    "    2. If no : what you should see to say that a classifier is overfitting?\n",
    "3. How does changing the parameters may affect the results?\n",
    "   (changing K in nearest neighbors, the hardness of the SVM, the gamma of the RBF kernel, the depth of the tree, the number of neurons of the Neural Networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72132d6c-debc-4192-81b1-a599bb2de9ef",
   "metadata": {},
   "source": [
    "Your answers here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d53475-4f98-4039-8222-9c16c0277499",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
